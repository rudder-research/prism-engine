{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÆ PRISM Engine - Full 14 Lenses\n",
    "\n",
    "One notebook. All lenses. No imports needed.\n",
    "\n",
    "**Run cells 1-4 in order, then explore.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title ‚öôÔ∏è **CELL 1: SETUP** (run first) { display-mode: \"form\" }\n#@markdown Sets up paths and loads all 14 lenses.\n\n# === SETUP - WORKS IN COLAB OR LOCALLY ===\nimport sys, os\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom scipy import stats\nfrom collections import defaultdict\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef find_prism_root():\n    \"\"\"Find prism-engine root - works in Colab or locally.\"\"\"\n    try:\n        from google.colab import drive\n        IN_COLAB = True\n    except ImportError:\n        IN_COLAB = False\n\n    if IN_COLAB:\n        drive.mount('/content/drive')\n        candidates = [\n            Path('/content/drive/MyDrive/prism-engine/prism-engine'),\n            Path('/content/drive/MyDrive/prism-engine'),\n            Path('/content/prism-engine'),\n        ]\n    else:\n        candidates = [\n            Path('.').resolve(),\n            Path('..').resolve(),\n        ]\n\n    for path in candidates:\n        if (path / '05_engine' / 'lenses').exists():\n            return path\n    return Path('.').resolve()\n\nPRISM_ROOT = find_prism_root()\nsys.path.insert(0, str(PRISM_ROOT))\nprint(f\"‚úì PRISM_ROOT = {PRISM_ROOT}\")\n\n# ============================================================================\n# ALL 14 LENSES - SELF-CONTAINED\n# ============================================================================\n\nclass BaseLens:\n    name = \"base\"\n    description = \"Base lens class\"\n    def analyze(self, panel): raise NotImplementedError\n\n\n# --- LENS 1: MAGNITUDE ---\nclass MagnitudeLens(BaseLens):\n    \"\"\"Importance by total magnitude of movement (L2 norm).\"\"\"\n    name = \"magnitude\"\n    def analyze(self, panel):\n        normalized = (panel - panel.mean()) / panel.std()\n        magnitude = np.sqrt((normalized ** 2).sum())\n        return {'importance': magnitude}\n\n\n# --- LENS 2: PCA ---\nclass PCALens(BaseLens):\n    \"\"\"Importance by contribution to principal components.\"\"\"\n    name = \"pca\"\n    def analyze(self, panel, n_components=5):\n        X = ((panel - panel.mean()) / panel.std()).fillna(0).values\n        U, S, Vt = np.linalg.svd(X, full_matrices=False)\n        explained = (S ** 2) / (len(X) - 1)\n        loadings = Vt[:n_components].T * S[:n_components]\n        importance = pd.Series(np.abs(loadings).sum(axis=1), index=panel.columns)\n        return {'importance': importance, 'explained_variance': explained[:n_components] / explained.sum()}\n\n\n# --- LENS 3: GRANGER CAUSALITY ---\nclass GrangerLens(BaseLens):\n    \"\"\"Importance by Granger-causal influence on other series.\"\"\"\n    name = \"granger\"\n    def analyze(self, panel, max_lag=5):\n        cols = panel.columns.tolist()\n        influence = pd.Series(0.0, index=cols)\n        \n        for target in cols:\n            y = panel[target].values\n            for source in cols:\n                if source == target: continue\n                x = panel[source].values\n                n = len(y) - max_lag\n                if n < 20: continue\n                Y = y[max_lag:]\n                X_restricted = np.column_stack([y[max_lag-i-1:-i-1] for i in range(max_lag)])\n                X_unrestricted = np.column_stack([X_restricted] + [x[max_lag-i-1:-i-1] for i in range(max_lag)])\n                try:\n                    rss_r = np.sum((Y - X_restricted @ np.linalg.lstsq(X_restricted, Y, rcond=None)[0])**2)\n                    rss_u = np.sum((Y - X_unrestricted @ np.linalg.lstsq(X_unrestricted, Y, rcond=None)[0])**2)\n                    f_stat = ((rss_r - rss_u) / max_lag) / (rss_u / (n - 2*max_lag))\n                    if f_stat > 2: influence[source] += f_stat\n                except: pass\n        importance = (influence - influence.min()) / (influence.max() - influence.min() + 1e-10)\n        return {'importance': importance, 'granger_influence': influence}\n\n\n# --- LENS 4: DMD ---\nclass DMDLens(BaseLens):\n    \"\"\"Importance by contribution to dominant dynamic modes.\"\"\"\n    name = \"dmd\"\n    def analyze(self, panel, n_modes=5):\n        X = ((panel - panel.mean()) / panel.std()).fillna(0).values\n        X1, X2 = X[:-1].T, X[1:].T\n        U, S, Vh = np.linalg.svd(X1, full_matrices=False)\n        r = min(n_modes, len(S))\n        U, S, Vh = U[:, :r], S[:r], Vh[:r, :]\n        Atilde = U.T @ X2 @ Vh.T @ np.diag(1/S)\n        eigs, W = np.linalg.eig(Atilde)\n        Phi = X2 @ Vh.T @ np.diag(1/S) @ W\n        importance = pd.Series(np.abs(Phi).sum(axis=1), index=panel.columns)\n        return {'importance': importance, 'eigenvalues': eigs}\n\n\n# --- LENS 5: INFLUENCE ---\nclass InfluenceLens(BaseLens):\n    \"\"\"Importance by volatility √ó deviation from mean.\"\"\"\n    name = \"influence\"\n    def analyze(self, panel, window=20):\n        vol = panel.rolling(window=window).std()\n        dev = np.abs(panel - panel.rolling(window=window).mean())\n        influence = (vol * dev).mean()\n        importance = (influence - influence.min()) / (influence.max() - influence.min() + 1e-10)\n        return {'importance': importance}\n\n\n# --- LENS 6: MUTUAL INFORMATION ---\nclass MutualInfoLens(BaseLens):\n    \"\"\"Importance by mutual information with other series.\"\"\"\n    name = \"mutual_info\"\n    def analyze(self, panel, n_bins=20):\n        cols = panel.columns.tolist()\n        mi_scores = pd.Series(0.0, index=cols)\n        for i, col1 in enumerate(cols):\n            x = panel[col1].values\n            for col2 in cols[i+1:]:\n                y = panel[col2].values\n                x_bins = np.digitize(x, np.linspace(x.min(), x.max(), n_bins))\n                y_bins = np.digitize(y, np.linspace(y.min(), y.max(), n_bins))\n                pxy = np.histogram2d(x_bins, y_bins, bins=n_bins)[0]\n                pxy = pxy / pxy.sum()\n                px, py = pxy.sum(axis=1), pxy.sum(axis=0)\n                mi = sum(pxy[xi, yi] * np.log(pxy[xi, yi] / (px[xi] * py[yi]))\n                         for xi in range(n_bins) for yi in range(n_bins)\n                         if pxy[xi, yi] > 0 and px[xi] > 0 and py[yi] > 0)\n                mi_scores[col1] += mi\n                mi_scores[col2] += mi\n        importance = (mi_scores - mi_scores.min()) / (mi_scores.max() - mi_scores.min() + 1e-10)\n        return {'importance': importance, 'mutual_info': mi_scores}\n\n\n# --- LENS 7: CLUSTERING ---\nclass ClusteringLens(BaseLens):\n    \"\"\"Importance by centrality within correlation clusters.\"\"\"\n    name = \"clustering\"\n    def analyze(self, panel):\n        corr = panel.corr().abs()\n        importance = (corr.sum() - 1) / (len(corr) - 1)\n        return {'importance': importance, 'correlation_matrix': corr}\n\n\n# --- LENS 8: DECOMPOSITION ---\nclass DecompositionLens(BaseLens):\n    \"\"\"Importance by trend vs noise ratio.\"\"\"\n    name = \"decomposition\"\n    def analyze(self, panel, period=252):\n        importance_vals = []\n        for col in panel.columns:\n            s = panel[col].dropna()\n            if len(s) < period * 2:\n                importance_vals.append(0)\n                continue\n            trend = s.rolling(window=period, center=True).mean()\n            importance_vals.append(trend.var() / s.var() if s.var() > 0 else 0)\n        return {'importance': pd.Series(importance_vals, index=panel.columns)}\n\n\n# --- LENS 9: WAVELET ---\nclass WaveletLens(BaseLens):\n    \"\"\"Importance by multi-scale variance.\"\"\"\n    name = \"wavelet\"\n    def analyze(self, panel, scales=[5, 20, 60, 120, 252]):\n        importance_vals = []\n        for col in panel.columns:\n            s = panel[col].dropna()\n            scale_vars = [((s - s.rolling(window=scale).mean()).var()) for scale in scales if len(s) > scale]\n            importance_vals.append(np.mean(scale_vars) if scale_vars else 0)\n        importance = pd.Series(importance_vals, index=panel.columns)\n        return {'importance': (importance - importance.min()) / (importance.max() - importance.min() + 1e-10)}\n\n\n# --- LENS 10: NETWORK ---\nclass NetworkLens(BaseLens):\n    \"\"\"Importance by network centrality.\"\"\"\n    name = \"network\"\n    def analyze(self, panel, threshold=0.5):\n        corr = panel.corr().abs()\n        adj = (corr > threshold).astype(int).values\n        np.fill_diagonal(adj, 0)\n        degree = adj.sum(axis=1)\n        try:\n            eigs, vecs = np.linalg.eig(adj.astype(float))\n            eigen_cent = np.abs(vecs[:, np.argmax(eigs.real)].real)\n        except:\n            eigen_cent = degree\n        combined = 0.5 * degree / (degree.max() + 1e-10) + 0.5 * eigen_cent / (eigen_cent.max() + 1e-10)\n        return {'importance': pd.Series(combined, index=panel.columns), 'degree': pd.Series(degree, index=panel.columns)}\n\n\n# --- LENS 11: REGIME SWITCHING ---\nclass RegimeSwitchingLens(BaseLens):\n    \"\"\"Importance by behavior difference across regimes.\"\"\"\n    name = \"regime\"\n    def analyze(self, panel):\n        X = ((panel - panel.mean()) / panel.std()).fillna(0).values\n        U, S, Vt = np.linalg.svd(X, full_matrices=False)\n        pc1 = U[:, 0] * S[0]\n        vol = pd.Series(pc1).rolling(20).std().fillna(0).values\n        regime = (vol > np.median(vol)).astype(int)\n        importance_vals = []\n        for col in panel.columns:\n            s = panel[col].values\n            mean_r0 = s[regime == 0].mean() if (regime == 0).sum() > 0 else 0\n            mean_r1 = s[regime == 1].mean() if (regime == 1).sum() > 0 else 0\n            importance_vals.append(abs(mean_r1 - mean_r0) / (s.std() + 1e-10))\n        return {'importance': pd.Series(importance_vals, index=panel.columns), 'regime_labels': regime}\n\n\n# --- LENS 12: ANOMALY ---\nclass AnomalyLens(BaseLens):\n    \"\"\"Importance by frequency of anomalous behavior.\"\"\"\n    name = \"anomaly\"\n    def analyze(self, panel, z_threshold=2.5):\n        z_scores = (panel - panel.mean()) / panel.std()\n        anomaly_rate = (z_scores.abs() > z_threshold).mean()\n        return {'importance': (anomaly_rate - anomaly_rate.min()) / (anomaly_rate.max() - anomaly_rate.min() + 1e-10), 'anomaly_rate': anomaly_rate}\n\n\n# --- LENS 13: TRANSFER ENTROPY ---\nclass TransferEntropyLens(BaseLens):\n    \"\"\"Importance by information flow to other series.\"\"\"\n    name = \"transfer_entropy\"\n    def analyze(self, panel, lag=1):\n        cols = panel.columns.tolist()\n        te_out = pd.Series(0.0, index=cols)\n        for source in cols:\n            x = panel[source].values\n            for target in cols:\n                if source == target: continue\n                y = panel[target].values\n                if len(y) > lag:\n                    y_curr, y_past, x_past = y[lag:], y[:-lag], x[:-lag]\n                    try:\n                        slope = np.cov(y_curr, y_past)[0,1] / (np.var(y_past) + 1e-10)\n                        residual = y_curr - slope * y_past\n                        te = abs(np.corrcoef(residual, x_past)[0,1])\n                        if not np.isnan(te): te_out[source] += te\n                    except: pass\n        return {'importance': (te_out - te_out.min()) / (te_out.max() - te_out.min() + 1e-10), 'transfer_entropy_out': te_out}\n\n\n# --- LENS 14: TDA ---\nclass TDALens(BaseLens):\n    \"\"\"Importance by topological persistence.\"\"\"\n    name = \"tda\"\n    def analyze(self, panel, embed_dim=3, delay=5, sample_size=200):\n        from scipy.spatial.distance import pdist\n        importance_vals = []\n        for col in panel.columns:\n            s = panel[col].dropna().values\n            if len(s) < embed_dim * delay + sample_size:\n                importance_vals.append(0)\n                continue\n            n = len(s) - (embed_dim - 1) * delay\n            embedded = np.array([s[i:i + embed_dim * delay:delay] for i in range(n)])\n            if len(embedded) > sample_size:\n                embedded = embedded[np.random.choice(len(embedded), sample_size, replace=False)]\n            dists = pdist(embedded)\n            importance_vals.append(dists.max() - dists.min() if len(dists) > 0 else 0)\n        importance = pd.Series(importance_vals, index=panel.columns)\n        return {'importance': (importance - importance.min()) / (importance.max() - importance.min() + 1e-10)}\n\n\n# ============================================================================\n# LENS REGISTRY & RUNNER\n# ============================================================================\n\nLENSES = {\n    'magnitude': MagnitudeLens, 'pca': PCALens, 'granger': GrangerLens, 'dmd': DMDLens,\n    'influence': InfluenceLens, 'mutual_info': MutualInfoLens, 'clustering': ClusteringLens,\n    'decomposition': DecompositionLens, 'wavelet': WaveletLens, 'network': NetworkLens,\n    'regime': RegimeSwitchingLens, 'anomaly': AnomalyLens, 'transfer_entropy': TransferEntropyLens,\n    'tda': TDALens,\n}\n\ndef run_lens(name, panel):\n    return LENSES[name]().analyze(panel)\n\ndef run_all_lenses(panel, names=None, verbose=True):\n    names = names or list(LENSES.keys())\n    results = {}\n    for name in names:\n        try:\n            if verbose: print(f\"  {name}...\", end=\" \")\n            results[name] = run_lens(name, panel)\n            if verbose: print(\"‚úì\")\n        except Exception as e:\n            if verbose: print(f\"‚úó ({e})\")\n    return results\n\ndef compute_consensus(results):\n    rankings = {}\n    for name, res in results.items():\n        if 'importance' in res and isinstance(res['importance'], pd.Series):\n            rankings[name] = res['importance'].rank(ascending=False)\n    if not rankings: return pd.DataFrame()\n    df = pd.DataFrame(rankings)\n    df['avg_rank'] = df.mean(axis=1)\n    df['std_rank'] = df.std(axis=1)\n    df['agreement'] = 1 / (1 + df['std_rank'])\n    return df.sort_values('avg_rank')\n\nprint(f\"‚úì Loaded {len(LENSES)} lenses: {list(LENSES.keys())}\")\nprint(\"\\nüîë Ready!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title üìä **CELL 2: LOAD DATA** { display-mode: \"form\" }\n#@markdown Loads your data panel.\n\ndata_path = PRISM_ROOT / 'data' / 'raw' / 'master_panel.csv'\n\nif data_path.exists():\n    panel_raw = pd.read_csv(data_path, index_col=0, parse_dates=True)\n    print(f\"‚úì Loaded {data_path}\")\nelse:\n    # Build from individual files\n    print(\"Building from individual CSVs...\")\n    raw_dir = PRISM_ROOT / 'data' / 'raw'\n    dfs = {}\n    for f in raw_dir.glob('*.csv'):\n        try:\n            df = pd.read_csv(f, index_col=0, parse_dates=True)\n            name = f.stem.upper()\n            for col in ['Close', 'Adj Close', 'VALUE', 'Value', df.columns[0]]:\n                if col in df.columns:\n                    dfs[name] = df[col]\n                    break\n        except: pass\n    panel_raw = pd.DataFrame(dfs)\n\n# Clean\npanel = panel_raw.ffill().bfill().dropna()\n\nprint(f\"\\nüìä Panel: {panel.shape[1]} indicators √ó {panel.shape[0]} days\")\nprint(f\"   {panel.index[0].strftime('%Y-%m-%d')} to {panel.index[-1].strftime('%Y-%m-%d')}\")\nprint(f\"   Columns: {list(panel.columns)[:10]}{'...' if len(panel.columns) > 10 else ''}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üöÄ **CELL 3: RUN ALL 14 LENSES** { display-mode: \"form\" }\n",
    "#@markdown This may take 1-2 minutes for large panels.\n",
    "\n",
    "print(\"Running all 14 lenses...\\n\")\n",
    "results = run_all_lenses(panel)\n",
    "\n",
    "print(f\"\\n‚úì {len(results)}/14 lenses completed\")\n",
    "\n",
    "# Consensus\n",
    "consensus = compute_consensus(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÜ TOP 10 INDICATORS BY CONSENSUS\")\n",
    "print(\"=\"*60)\n",
    "for i, (ind, row) in enumerate(consensus.head(10).iterrows(), 1):\n",
    "    bar = \"‚ñà\" * int(row['agreement'] * 20)\n",
    "    print(f\"{i:2}. {ind:<15} rank={row['avg_rank']:5.1f}  agreement={row['agreement']:.2f} {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üìà **CELL 4: VISUALIZE RESULTS** { display-mode: \"form\" }\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Top indicators\n",
    "top15 = consensus.head(15)\n",
    "colors = plt.cm.RdYlGn(top15['agreement'].values)\n",
    "axes[0,0].barh(range(len(top15)), top15['avg_rank'].values[::-1], color=colors[::-1])\n",
    "axes[0,0].set_yticks(range(len(top15)))\n",
    "axes[0,0].set_yticklabels(top15.index[::-1])\n",
    "axes[0,0].set_xlabel('Avg Rank (lower=more important)')\n",
    "axes[0,0].set_title('Top 15 Indicators')\n",
    "axes[0,0].invert_xaxis()\n",
    "\n",
    "# 2. Lens agreement heatmap\n",
    "lens_cols = [c for c in consensus.columns if c not in ['avg_rank', 'std_rank', 'agreement']]\n",
    "top10_ranks = consensus[lens_cols].head(10)\n",
    "im = axes[0,1].imshow(top10_ranks.values, cmap='RdYlGn_r', aspect='auto')\n",
    "axes[0,1].set_xticks(range(len(lens_cols)))\n",
    "axes[0,1].set_xticklabels(lens_cols, rotation=45, ha='right', fontsize=8)\n",
    "axes[0,1].set_yticks(range(len(top10_ranks)))\n",
    "axes[0,1].set_yticklabels(top10_ranks.index)\n",
    "axes[0,1].set_title('Rank by Each Lens')\n",
    "plt.colorbar(im, ax=axes[0,1])\n",
    "\n",
    "# 3. Lens correlation\n",
    "lens_corr = consensus[lens_cols].corr(method='spearman')\n",
    "im2 = axes[1,0].imshow(lens_corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "axes[1,0].set_xticks(range(len(lens_cols)))\n",
    "axes[1,0].set_xticklabels(lens_cols, rotation=45, ha='right', fontsize=8)\n",
    "axes[1,0].set_yticks(range(len(lens_cols)))\n",
    "axes[1,0].set_yticklabels(lens_cols, fontsize=8)\n",
    "axes[1,0].set_title('Lens Agreement (Spearman œÅ)')\n",
    "plt.colorbar(im2, ax=axes[1,0])\n",
    "\n",
    "# 4. Agreement distribution\n",
    "axes[1,1].hist(consensus['agreement'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[1,1].axvline(consensus['agreement'].median(), color='red', linestyle='--', label=f\"Median={consensus['agreement'].median():.2f}\")\n",
    "axes[1,1].set_xlabel('Agreement Score')\n",
    "axes[1,1].set_ylabel('Count')\n",
    "axes[1,1].set_title('Distribution of Lens Agreement')\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print lens correlations\n",
    "print(\"\\nLens Agreement Matrix (Spearman):\")\n",
    "print(lens_corr.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title üíæ **SAVE RESULTS** { display-mode: \"form\" }\n\noutput_dir = PRISM_ROOT / '06_output' / 'latest'\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nconsensus.to_csv(output_dir / 'consensus_14lens.csv')\n\nimport json\nmeta = {\n    'timestamp': datetime.now().isoformat(),\n    'n_indicators': len(panel.columns),\n    'n_days': len(panel),\n    'lenses_run': list(results.keys()),\n    'top_10': list(consensus.head(10).index),\n}\nwith open(output_dir / 'run_14lens.json', 'w') as f:\n    json.dump(meta, f, indent=2)\n\nprint(f\"‚úì Saved to {output_dir}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore a specific lens\n",
    "lens_name = 'granger'  # Change this!\n",
    "\n",
    "result = results[lens_name]\n",
    "print(f\"\\n{lens_name.upper()} LENS\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Keys: {list(result.keys())}\")\n",
    "print(f\"\\nTop 10:\")\n",
    "print(result['importance'].sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find disagreements - where lenses differ most\n",
    "high_variance = consensus.nlargest(10, 'std_rank')[['avg_rank', 'std_rank', 'agreement']]\n",
    "print(\"\\nü§î MOST CONTESTED INDICATORS (lenses disagree)\")\n",
    "print(\"=\"*50)\n",
    "print(high_variance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}