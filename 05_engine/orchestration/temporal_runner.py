#!/usr/bin/env python3
"""
PRISM Temporal Runner - Year-Based Windowed Analysis
=====================================================

Run all 14 PRISM lenses across discrete time windows to track how
indicator importance changes over different periods.

Usage (command line):
    python temporal_runner.py --start 2005 --end 2025 --increment 5
    python temporal_runner.py --increment 8
    python temporal_runner.py --start 2010 --end 2024 --increment 2 --parallel

Usage (Python):
    from temporal_runner import TemporalRunner
    runner = TemporalRunner(start_year=2005, end_year=2025, increment=5)
    results = runner.run()

Output Files (in 06_output/temporal/):
    - temporal_results_{increment}yr.csv : Long format with all windows
    - rank_evolution_{increment}yr.csv   : Pivot table (indicators × windows)
"""

import sys
import os
from pathlib import Path
import argparse
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from concurrent.futures import ProcessPoolExecutor, as_completed
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np

# Ensure project root is in path
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


# =============================================================================
# LENS CONFIGURATION
# =============================================================================

ALL_LENSES = [
    'magnitude',
    'pca',
    'influence',
    'clustering',
    'decomposition',
    'granger',
    'dmd',
    'mutual_info',
    'network',
    'regime_switching',
    'anomaly',
    'transfer_entropy',
    'tda',
    'wavelet',
]


def load_lens_class(lens_name: str):
    """Dynamically load a lens class by name."""
    lens_map = {
        'magnitude': ('magnitude_lens', 'MagnitudeLens'),
        'pca': ('pca_lens', 'PCALens'),
        'influence': ('influence_lens', 'InfluenceLens'),
        'clustering': ('clustering_lens', 'ClusteringLens'),
        'decomposition': ('decomposition_lens', 'DecompositionLens'),
        'granger': ('granger_lens', 'GrangerLens'),
        'dmd': ('dmd_lens', 'DMDLens'),
        'mutual_info': ('mutual_info_lens', 'MutualInfoLens'),
        'network': ('network_lens', 'NetworkLens'),
        'regime_switching': ('regime_switching_lens', 'RegimeSwitchingLens'),
        'anomaly': ('anomaly_lens', 'AnomalyLens'),
        'transfer_entropy': ('transfer_entropy_lens', 'TransferEntropyLens'),
        'tda': ('tda_lens', 'TDALens'),
        'wavelet': ('wavelet_lens', 'WaveletLens'),
    }

    if lens_name not in lens_map:
        raise ValueError(f"Unknown lens: {lens_name}")

    module_name, class_name = lens_map[lens_name]

    try:
        from importlib import import_module
        module = import_module(f"05_engine.lenses.{module_name}")
        return getattr(module, class_name)
    except ImportError as e:
        # Try alternate import path
        try:
            lenses_path = PROJECT_ROOT / "05_engine" / "lenses"
            sys.path.insert(0, str(lenses_path))
            module = import_module(module_name)
            return getattr(module, class_name)
        except Exception:
            raise ImportError(f"Could not load lens {lens_name}: {e}")


def run_single_lens(args: Tuple[str, pd.DataFrame]) -> Tuple[str, Optional[pd.DataFrame]]:
    """
    Run a single lens on data. Used for parallel execution.

    Args:
        args: Tuple of (lens_name, dataframe)

    Returns:
        Tuple of (lens_name, ranking_dataframe or None)
    """
    lens_name, df = args
    try:
        LensClass = load_lens_class(lens_name)
        lens = LensClass()
        ranking = lens.rank_indicators(df)
        return (lens_name, ranking)
    except Exception as e:
        return (lens_name, None)


# =============================================================================
# TEMPORAL RUNNER CLASS
# =============================================================================

class TemporalRunner:
    """
    Run PRISM analysis across discrete time windows.

    Generates windows like: 2005-2010, 2010-2015, 2015-2020, etc.
    """

    def __init__(
        self,
        start_year: int = 2005,
        end_year: int = None,
        increment: int = 5,
        data_path: str = None,
        output_dir: str = None,
        lenses: List[str] = None,
        parallel: bool = False,
        max_workers: int = None,
    ):
        """
        Initialize temporal runner.

        Args:
            start_year: First year to include (default 2005)
            end_year: Last year to include (default: current year)
            increment: Years per window (default 5)
            data_path: Path to master_panel.csv (default: data/raw/master_panel.csv)
            output_dir: Output directory (default: 06_output/temporal/)
            lenses: List of lenses to run (default: all 14)
            parallel: Use parallel processing for lenses
            max_workers: Max parallel workers (default: CPU count)
        """
        self.start_year = start_year
        self.end_year = end_year or datetime.now().year
        self.increment = increment
        self.parallel = parallel
        self.max_workers = max_workers

        # Paths
        self.project_root = PROJECT_ROOT
        self.data_path = Path(data_path) if data_path else self.project_root / "data" / "raw" / "master_panel.csv"
        self.output_dir = Path(output_dir) if output_dir else self.project_root / "06_output" / "temporal"

        # Lenses
        self.lenses = lenses or ALL_LENSES

        # Storage
        self.panel: Optional[pd.DataFrame] = None
        self.windows: List[Tuple[int, int]] = []
        self.results: Dict[str, Any] = {}

    def generate_windows(self) -> List[Tuple[int, int]]:
        """Generate time windows based on increment."""
        windows = []
        current = self.start_year

        while current < self.end_year:
            window_end = min(current + self.increment, self.end_year)
            windows.append((current, window_end))
            current = window_end

        self.windows = windows
        return windows

    def load_data(self) -> pd.DataFrame:
        """Load and prepare master panel data."""
        print(f"Loading data from {self.data_path}...")

        if not self.data_path.exists():
            raise FileNotFoundError(f"Data file not found: {self.data_path}")

        # Load with date parsing
        panel = pd.read_csv(self.data_path, index_col=0, parse_dates=True)

        # Ensure datetime index
        if not isinstance(panel.index, pd.DatetimeIndex):
            panel.index = pd.to_datetime(panel.index)

        panel = panel.sort_index()

        print(f"  Loaded: {panel.shape[0]} rows, {panel.shape[1]} columns")
        print(f"  Date range: {panel.index[0].date()} to {panel.index[-1].date()}")

        self.panel = panel
        return panel

    def slice_by_window(self, start_year: int, end_year: int) -> pd.DataFrame:
        """Extract data for a specific time window."""
        start_date = f"{start_year}-01-01"
        end_date = f"{end_year}-12-31"

        window_data = self.panel.loc[start_date:end_date].copy()

        # Clean: forward fill, backward fill, drop remaining NaN columns
        window_data = window_data.ffill().bfill()

        # Drop columns that are all NaN in this window
        window_data = window_data.dropna(axis=1, how='all')

        # Drop rows with any remaining NaN
        window_data = window_data.dropna()

        return window_data

    def prepare_for_lens(self, df: pd.DataFrame) -> pd.DataFrame:
        """Prepare DataFrame in format expected by lenses (with 'date' column)."""
        result = df.reset_index()
        result.columns = ['date'] + list(result.columns[1:])
        return result

    def run_lenses_on_window(
        self,
        window_data: pd.DataFrame,
        window_label: str
    ) -> Dict[str, pd.DataFrame]:
        """
        Run all lenses on a single window.

        Args:
            window_data: Data for this window
            window_label: Label like "2005-2010"

        Returns:
            Dict mapping lens_name -> ranking DataFrame
        """
        prepared = self.prepare_for_lens(window_data)
        rankings = {}

        if self.parallel:
            # Parallel execution
            with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {
                    executor.submit(run_single_lens, (lens, prepared)): lens
                    for lens in self.lenses
                }

                for future in as_completed(futures):
                    lens_name, ranking = future.result()
                    if ranking is not None:
                        rankings[lens_name] = ranking
        else:
            # Sequential execution
            for lens_name in self.lenses:
                try:
                    LensClass = load_lens_class(lens_name)
                    lens = LensClass()
                    ranking = lens.rank_indicators(prepared)
                    rankings[lens_name] = ranking
                except Exception as e:
                    print(f"    Warning: {lens_name} failed: {e}")

        return rankings

    def compute_consensus(self, rankings: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """
        Compute consensus ranking from multiple lens rankings.

        Uses Borda count: each indicator gets points = n - rank
        """
        if not rankings:
            return pd.DataFrame()

        # Collect all indicators
        all_indicators = set()
        for ranking_df in rankings.values():
            if 'indicator' in ranking_df.columns:
                all_indicators.update(ranking_df['indicator'].tolist())

        n = len(all_indicators)
        scores = {ind: 0 for ind in all_indicators}
        lens_count = {ind: 0 for ind in all_indicators}

        # Borda count scoring
        for lens_name, ranking_df in rankings.items():
            if 'indicator' not in ranking_df.columns or 'rank' not in ranking_df.columns:
                continue

            for _, row in ranking_df.iterrows():
                ind = row['indicator']
                rank = row['rank']
                scores[ind] += (n - rank)
                lens_count[ind] += 1

        # Build consensus DataFrame
        consensus = []
        for ind in all_indicators:
            avg_score = scores[ind] / lens_count[ind] if lens_count[ind] > 0 else 0
            consensus.append({
                'indicator': ind,
                'consensus_score': avg_score,
                'n_lenses': lens_count[ind],
            })

        consensus_df = pd.DataFrame(consensus)
        consensus_df = consensus_df.sort_values('consensus_score', ascending=False)
        consensus_df['consensus_rank'] = range(1, len(consensus_df) + 1)

        return consensus_df.reset_index(drop=True)

    def run(self, verbose: bool = True) -> Dict[str, Any]:
        """
        Run the full temporal analysis.

        Returns:
            Dictionary with all results
        """
        start_time = datetime.now()

        if verbose:
            print("=" * 60)
            print("PRISM TEMPORAL ANALYSIS")
            print("=" * 60)
            print(f"Start year:  {self.start_year}")
            print(f"End year:    {self.end_year}")
            print(f"Increment:   {self.increment} years")
            print(f"Parallel:    {self.parallel}")
            print(f"Lenses:      {len(self.lenses)}")
            print()

        # Load data
        self.load_data()

        # Generate windows
        windows = self.generate_windows()
        if verbose:
            print(f"\nGenerated {len(windows)} time windows:")
            for start, end in windows:
                print(f"  {start}-{end}")
            print()

        # Process each window
        all_results = []
        window_rankings = {}

        for i, (start, end) in enumerate(windows):
            window_label = f"{start}-{end}"

            if verbose:
                print(f"[{i+1}/{len(windows)}] Processing {window_label}...")

            # Slice data
            window_data = self.slice_by_window(start, end)

            if len(window_data) < 100:
                if verbose:
                    print(f"    Skipping: insufficient data ({len(window_data)} rows)")
                continue

            if verbose:
                print(f"    Data: {len(window_data)} rows, {len(window_data.columns)} indicators")

            # Run lenses
            rankings = self.run_lenses_on_window(window_data, window_label)

            if verbose:
                print(f"    Lenses completed: {len(rankings)}/{len(self.lenses)}")

            # Compute consensus
            consensus = self.compute_consensus(rankings)

            # Store results
            window_rankings[window_label] = consensus

            # Add to long-format results
            for _, row in consensus.iterrows():
                all_results.append({
                    'window': window_label,
                    'start_year': start,
                    'end_year': end,
                    'indicator': row['indicator'],
                    'consensus_rank': row['consensus_rank'],
                    'consensus_score': row['consensus_score'],
                    'n_lenses': row['n_lenses'],
                })

        # Build output DataFrames
        results_df = pd.DataFrame(all_results)

        # Pivot for rank evolution (indicators × windows)
        if not results_df.empty:
            rank_evolution = results_df.pivot(
                index='indicator',
                columns='window',
                values='consensus_rank'
            )
            # Sort by average rank
            rank_evolution['avg_rank'] = rank_evolution.mean(axis=1)
            rank_evolution = rank_evolution.sort_values('avg_rank')
        else:
            rank_evolution = pd.DataFrame()

        # Store results
        self.results = {
            'long_format': results_df,
            'rank_evolution': rank_evolution,
            'window_rankings': window_rankings,
            'windows': windows,
            'metadata': {
                'start_year': self.start_year,
                'end_year': self.end_year,
                'increment': self.increment,
                'n_windows': len(windows),
                'lenses': self.lenses,
                'run_time': str(datetime.now() - start_time),
            }
        }

        # Save outputs
        self.save_results(verbose=verbose)

        if verbose:
            elapsed = datetime.now() - start_time
            print()
            print("=" * 60)
            print(f"COMPLETE - {elapsed}")
            print("=" * 60)

            if not rank_evolution.empty:
                print("\nTOP 10 INDICATORS (by average rank across all windows):")
                print(rank_evolution.head(10)[['avg_rank']].to_string())

        return self.results

    def save_results(self, verbose: bool = True):
        """Save results to CSV files."""
        self.output_dir.mkdir(parents=True, exist_ok=True)

        increment = self.increment

        # Long format
        long_path = self.output_dir / f"temporal_results_{increment}yr.csv"
        self.results['long_format'].to_csv(long_path, index=False)

        # Rank evolution pivot
        evolution_path = self.output_dir / f"rank_evolution_{increment}yr.csv"
        self.results['rank_evolution'].to_csv(evolution_path)

        if verbose:
            print(f"\nSaved results to:")
            print(f"  {long_path}")
            print(f"  {evolution_path}")


# =============================================================================
# COMMAND LINE INTERFACE
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="PRISM Temporal Analysis - Run lenses across time windows",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python temporal_runner.py --increment 5
  python temporal_runner.py --start 2005 --end 2025 --increment 5
  python temporal_runner.py --increment 8 --parallel
  python temporal_runner.py --start 2010 --end 2024 --increment 2 --lenses magnitude pca influence
        """
    )

    parser.add_argument(
        '--start', '-s',
        type=int,
        default=2005,
        help='Start year (default: 2005)'
    )

    parser.add_argument(
        '--end', '-e',
        type=int,
        default=None,
        help='End year (default: current year)'
    )

    parser.add_argument(
        '--increment', '-i',
        type=int,
        default=5,
        help='Years per window (default: 5)'
    )

    parser.add_argument(
        '--data',
        type=str,
        default=None,
        help='Path to master_panel.csv'
    )

    parser.add_argument(
        '--output', '-o',
        type=str,
        default=None,
        help='Output directory'
    )

    parser.add_argument(
        '--parallel', '-p',
        action='store_true',
        help='Use parallel processing for lenses'
    )

    parser.add_argument(
        '--workers', '-w',
        type=int,
        default=None,
        help='Max parallel workers (default: CPU count)'
    )

    parser.add_argument(
        '--lenses', '-l',
        nargs='+',
        default=None,
        help='Specific lenses to run (default: all 14)'
    )

    parser.add_argument(
        '--list-lenses',
        action='store_true',
        help='List available lenses and exit'
    )

    args = parser.parse_args()

    # List lenses option
    if args.list_lenses:
        print("Available lenses:")
        for lens in ALL_LENSES:
            print(f"  - {lens}")
        return

    # Create runner
    runner = TemporalRunner(
        start_year=args.start,
        end_year=args.end,
        increment=args.increment,
        data_path=args.data,
        output_dir=args.output,
        lenses=args.lenses,
        parallel=args.parallel,
        max_workers=args.workers,
    )

    # Run
    runner.run()


if __name__ == "__main__":
    main()
