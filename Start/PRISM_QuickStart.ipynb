{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”® PRISM Engine - Quick Start\n",
    "\n",
    "This notebook gets you running in 60 seconds.\n",
    "\n",
    "**Steps:**\n",
    "1. Mount Google Drive\n",
    "2. Set up paths\n",
    "3. Run the engine\n",
    "4. See results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Mount Drive & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === SETUP - WORKS IN COLAB OR LOCALLY ===\nimport sys\nfrom pathlib import Path\n\ndef find_prism_root():\n    \"\"\"Find prism-engine root - works in Colab or locally.\"\"\"\n    try:\n        from google.colab import drive\n        IN_COLAB = True\n    except ImportError:\n        IN_COLAB = False\n\n    if IN_COLAB:\n        drive.mount('/content/drive')\n        candidates = [\n            Path('/content/drive/MyDrive/prism-engine/prism-engine'),\n            Path('/content/drive/MyDrive/prism-engine'),\n            Path('/content/prism-engine'),\n        ]\n    else:\n        # Local: check current dir, parent (if in Start/), etc.\n        candidates = [\n            Path('.').resolve(),\n            Path('..').resolve(),  # If running from Start/\n        ]\n\n    for path in candidates:\n        if (path / '05_engine' / 'lenses').exists():\n            return path\n    return Path('.').resolve()\n\nPRISM_ROOT = find_prism_root()\nsys.path.insert(0, str(PRISM_ROOT))\n\nprint(f\"âœ“ PRISM_ROOT = {PRISM_ROOT}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify the structure exists\nimport os\n\nexpected_folders = ['01_fetch', '02_data_raw', '03_cleaning', '04_data_clean', \n                    '05_engine', '06_output', '07_interpretation', '08_visualization']\n\nprint(\"Checking folder structure...\")\nfor folder in expected_folders:\n    path = PRISM_ROOT / folder\n    status = \"âœ“\" if path.exists() else \"âœ—\"\n    print(f\"  {status} {folder}\")\n\n# Check for data\ndata_path = PRISM_ROOT / 'data' / 'raw'\nif data_path.exists():\n    csvs = list(data_path.glob('*.csv'))\n    print(f\"\\nâœ“ Found {len(csvs)} CSV files in data/raw/\")\nelse:\n    print(\"\\nâœ— No data/raw/ folder found\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\n\n# Try to load master_panel.csv or build it\nmaster_path = PRISM_ROOT / 'data' / 'raw' / 'master_panel.csv'\n\nif master_path.exists():\n    panel = pd.read_csv(master_path, index_col=0, parse_dates=True)\n    print(f\"âœ“ Loaded master_panel.csv\")\nelse:\n    # Build from individual CSVs\n    print(\"Building panel from individual CSVs...\")\n    raw_dir = PRISM_ROOT / 'data' / 'raw'\n    \n    dfs = {}\n    for f in raw_dir.glob('*.csv'):\n        if f.name != 'master_panel.csv':\n            try:\n                df = pd.read_csv(f, index_col=0, parse_dates=True)\n                name = f.stem.upper()\n                if len(df.columns) == 1:\n                    dfs[name] = df.iloc[:, 0]\n                else:\n                    # Use first numeric column\n                    for col in df.columns:\n                        if df[col].dtype in ['float64', 'int64']:\n                            dfs[name] = df[col]\n                            break\n            except:\n                pass\n    \n    panel = pd.DataFrame(dfs)\n    print(f\"âœ“ Built panel from {len(dfs)} files\")\n\nprint(f\"\\nPanel shape: {panel.shape}\")\nprint(f\"Date range: {panel.index[0]} to {panel.index[-1]}\")\nprint(f\"Columns: {list(panel.columns)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at the data\n",
    "panel.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaNs\n",
    "nan_pct = (panel.isna().sum() / len(panel) * 100).round(1)\n",
    "print(\"NaN % by column:\")\n",
    "print(nan_pct[nan_pct > 0].sort_values(ascending=False) if nan_pct.any() else \"No NaNs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple cleaning: forward fill, backward fill, drop remaining NaNs\n",
    "panel_clean = panel.ffill().bfill().dropna()\n",
    "\n",
    "print(f\"Before cleaning: {panel.shape}\")\n",
    "print(f\"After cleaning:  {panel_clean.shape}\")\n",
    "print(f\"NaNs remaining:  {panel_clean.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run a Single Lens (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test with the simplest lens first - Magnitude\ntry:\n    from engine.lenses.magnitude_lens import MagnitudeLens\n    print(\"Import style 1 worked\")\nexcept:\n    try:\n        from prism_engine.engine.lenses.magnitude_lens import MagnitudeLens  \n        print(\"Import style 2 worked\")\n    except:\n        # Direct import\n        exec(open(PRISM_ROOT / '05_engine' / 'lenses' / 'magnitude_lens.py').read())\n        print(\"Direct exec worked\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# If imports are tricky, here's a universal loader\nimport importlib.util\n\ndef load_lens(lens_name):\n    \"\"\"Load a lens by name, handling path issues.\"\"\"\n    lens_path = PRISM_ROOT / '05_engine' / 'lenses' / f'{lens_name}_lens.py'\n    \n    if not lens_path.exists():\n        raise FileNotFoundError(f\"Lens not found: {lens_path}\")\n    \n    spec = importlib.util.spec_from_file_location(lens_name, lens_path)\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n    \n    # Find the lens class (assumes it ends with 'Lens')\n    for name in dir(module):\n        if name.endswith('Lens') and name != 'BaseLens':\n            return getattr(module, name)\n    \n    raise ValueError(f\"No Lens class found in {lens_name}\")\n\nprint(\"âœ“ Lens loader ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and run Magnitude Lens\n",
    "MagnitudeLens = load_lens('magnitude')\n",
    "mag_lens = MagnitudeLens()\n",
    "\n",
    "print(\"Running MagnitudeLens...\")\n",
    "mag_result = mag_lens.analyze(panel_clean)\n",
    "\n",
    "print(\"\\nResult keys:\", list(mag_result.keys()))\n",
    "\n",
    "if 'importance' in mag_result:\n",
    "    imp = mag_result['importance']\n",
    "    if isinstance(imp, pd.Series):\n",
    "        print(\"\\nTop 10 by Magnitude:\")\n",
    "        print(imp.sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Multiple Lenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all available lenses\n",
    "lens_names = ['magnitude', 'pca', 'influence', 'clustering', 'decomposition']\n",
    "\n",
    "results = {}\n",
    "for name in lens_names:\n",
    "    try:\n",
    "        print(f\"Running {name}...\", end=\" \")\n",
    "        LensClass = load_lens(name)\n",
    "        lens = LensClass()\n",
    "        results[name] = lens.analyze(panel_clean)\n",
    "        print(\"âœ“\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— ({e})\")\n",
    "\n",
    "print(f\"\\nSuccessfully ran {len(results)}/{len(lens_names)} lenses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Compare Results (Consensus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build consensus from all lenses\n",
    "rankings = {}\n",
    "\n",
    "for lens_name, result in results.items():\n",
    "    if 'importance' in result:\n",
    "        imp = result['importance']\n",
    "        if isinstance(imp, pd.Series):\n",
    "            # Convert to ranks (1 = most important)\n",
    "            ranks = imp.rank(ascending=False)\n",
    "            rankings[lens_name] = ranks\n",
    "\n",
    "if rankings:\n",
    "    # Combine into DataFrame\n",
    "    rank_df = pd.DataFrame(rankings)\n",
    "    \n",
    "    # Average rank across lenses\n",
    "    rank_df['avg_rank'] = rank_df.mean(axis=1)\n",
    "    rank_df['std_rank'] = rank_df.std(axis=1)\n",
    "    \n",
    "    # Sort by average rank\n",
    "    consensus = rank_df.sort_values('avg_rank')\n",
    "    \n",
    "    print(\"CONSENSUS RANKINGS (lower = more important)\")\n",
    "    print(\"=\"*60)\n",
    "    print(consensus.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize agreement between lenses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if len(rankings) > 1:\n",
    "    # Correlation matrix of rankings\n",
    "    rank_df_lenses = pd.DataFrame(rankings)\n",
    "    corr = rank_df_lenses.corr(method='spearman')\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(corr, cmap='RdYlGn', vmin=-1, vmax=1)\n",
    "    plt.colorbar(label='Spearman Correlation')\n",
    "    plt.xticks(range(len(corr)), corr.columns, rotation=45, ha='right')\n",
    "    plt.yticks(range(len(corr)), corr.columns)\n",
    "    plt.title('Lens Agreement Matrix')\n",
    "    \n",
    "    # Add correlation values\n",
    "    for i in range(len(corr)):\n",
    "        for j in range(len(corr)):\n",
    "            plt.text(j, i, f'{corr.iloc[i, j]:.2f}', ha='center', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nAverage lens agreement: {corr.values[np.triu_indices_from(corr.values, 1)].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save consensus to output folder\noutput_dir = PRISM_ROOT / '06_output' / 'latest'\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nif 'consensus' in dir():\n    consensus.to_csv(output_dir / 'consensus_rankings.csv')\n    print(f\"âœ“ Saved consensus to {output_dir}/consensus_rankings.csv\")\n\n# Save run info\nimport json\nfrom datetime import datetime\n\nrun_info = {\n    'timestamp': datetime.now().isoformat(),\n    'data_shape': list(panel_clean.shape),\n    'lenses_run': list(results.keys()),\n    'top_5_indicators': list(consensus.head(5).index) if 'consensus' in dir() else []\n}\n\nwith open(output_dir / 'run_info.json', 'w') as f:\n    json.dump(run_info, f, indent=2)\n\nprint(f\"âœ“ Saved run info to {output_dir}/run_info.json\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ You Did It!\n",
    "\n",
    "**What just happened:**\n",
    "1. Loaded your data\n",
    "2. Cleaned NaNs\n",
    "3. Ran multiple mathematical lenses\n",
    "4. Computed consensus rankings\n",
    "5. Visualized lens agreement\n",
    "6. Saved results\n",
    "\n",
    "**Next steps:**\n",
    "- Try more lenses: `'wavelet'`, `'network'`, `'regime_switching'`, `'tda'`\n",
    "- Run validation tests\n",
    "- Explore disagreements between lenses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}