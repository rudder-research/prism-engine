# =============================================================================
# PRISM AI Interpretation Configuration
# =============================================================================
#
# This file configures the AI interpretation layer for PRISM.
# Copy to 'interpretation_config.local.yaml' for local overrides.

# -----------------------------------------------------------------------------
# Default Settings
# -----------------------------------------------------------------------------
defaults:
  # AI backend to use: manual, claude, openai, ollama
  backend: manual

  # Whether to save interpretations to database
  save_interpretations: true

  # Whether to save prompts (for debugging/analysis)
  save_prompts: true

  # Maximum tokens for AI responses
  max_tokens: 2000

  # Temperature for AI generation (lower = more focused)
  temperature: 0.3

# -----------------------------------------------------------------------------
# Backend-Specific Configuration
# -----------------------------------------------------------------------------
backends:
  # Anthropic Claude configuration
  claude:
    # Model to use (claude-sonnet-4-20250514 recommended for interpretation)
    model: claude-sonnet-4-20250514

    # Maximum tokens in response
    max_tokens: 2000

    # Temperature (0.0-1.0, lower = more deterministic)
    temperature: 0.3

    # API key (set via ANTHROPIC_API_KEY environment variable)
    # api_key: ${ANTHROPIC_API_KEY}

  # OpenAI configuration
  openai:
    # Model to use
    model: gpt-4o

    # Maximum tokens in response
    max_tokens: 2000

    # Temperature
    temperature: 0.3

    # API key (set via OPENAI_API_KEY environment variable)
    # api_key: ${OPENAI_API_KEY}

  # Local Ollama configuration
  ollama:
    # Model to use (must be pulled locally)
    model: llama3.1:70b

    # Ollama API endpoint
    endpoint: http://localhost:11434

    # Request timeout in seconds
    timeout: 120

  # Manual mode (returns prompts for human submission)
  manual:
    # No configuration needed
    # Prompts will be formatted for copy/paste

# -----------------------------------------------------------------------------
# Prompt Settings
# -----------------------------------------------------------------------------
prompt_settings:
  # Include historical context in prompts
  include_historical_context: true

  # Maximum number of similar events to include for context
  max_similar_events: 5

  # Include raw data samples in prompts (may affect privacy)
  include_raw_data_sample: false

  # Number of top indicators to include in window summaries
  top_n_indicators: 10

  # Number of bottom indicators to include
  bottom_n_indicators: 5

  # Maximum rank changes to show in regime break analysis
  max_rank_changes: 15

# -----------------------------------------------------------------------------
# Feedback Settings
# -----------------------------------------------------------------------------
feedback:
  # Require human validation before patterns are reused
  require_validation_for_patterns: true

  # Notify on rejection (for prompt improvement workflow)
  notify_on_rejection: false

  # Minimum number of validations before pattern is considered stable
  min_validations_for_stable: 3

  # Export format for training data
  export_format: jsonl

# -----------------------------------------------------------------------------
# Database Settings
# -----------------------------------------------------------------------------
database:
  # Path to temporal database (relative to project root)
  temporal_db_path: 06_output/temporal/temporal.db

  # Path to PRISM database (for indicator metadata)
  prism_db_path: data/sql/prism.db

  # Auto-run migrations on startup
  auto_migrate: false

# -----------------------------------------------------------------------------
# Logging Settings
# -----------------------------------------------------------------------------
logging:
  # Log level: DEBUG, INFO, WARNING, ERROR
  level: INFO

  # Log interpretation calls
  log_calls: true

  # Log prompts (verbose)
  log_prompts: false

  # Log responses (verbose)
  log_responses: false

# -----------------------------------------------------------------------------
# Rate Limiting (for API backends)
# -----------------------------------------------------------------------------
rate_limiting:
  # Enable rate limiting
  enabled: true

  # Maximum requests per minute
  requests_per_minute: 10

  # Delay between requests (seconds)
  delay_between_requests: 0.5

# -----------------------------------------------------------------------------
# Caching Settings
# -----------------------------------------------------------------------------
caching:
  # Enable response caching
  enabled: false

  # Cache TTL in seconds (1 hour)
  ttl_seconds: 3600

  # Cache directory
  cache_dir: .cache/interpretations
