{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”® PRISM Engine\n",
    "\n",
    "**One notebook. That's it.**\n",
    "\n",
    "Run cells top to bottom. Results at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title ðŸ”‘ **RUN THIS FIRST** (click play, wait for âœ“) { display-mode: \"form\" }\n#@markdown This cell sets up paths and loads everything.\n\n# === SETUP - WORKS IN COLAB OR LOCALLY ===\nimport sys\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef find_prism_root():\n    \"\"\"Find prism-engine root - works in Colab or locally.\"\"\"\n    # Check if we're in Colab\n    try:\n        from google.colab import drive\n        IN_COLAB = True\n    except ImportError:\n        IN_COLAB = False\n\n    if IN_COLAB:\n        # Mount drive if in Colab\n        drive.mount('/content/drive')\n        # Search common locations\n        candidates = [\n            Path('/content/drive/MyDrive/prism-engine/prism-engine'),\n            Path('/content/drive/MyDrive/prism-engine'),\n            Path('/content/prism-engine'),\n        ]\n    else:\n        # Local execution - search relative to notebook or cwd\n        candidates = [\n            Path('.').resolve(),\n            Path('..').resolve(),\n            Path(__file__).parent.resolve() if '__file__' in dir() else Path('.').resolve(),\n        ]\n\n    # Find the one with 05_engine/lenses\n    for path in candidates:\n        if (path / '05_engine' / 'lenses').exists():\n            return path\n\n    # Fallback: current directory\n    return Path('.').resolve()\n\nPRISM_ROOT = find_prism_root()\nsys.path.insert(0, str(PRISM_ROOT))\nprint(f\"âœ“ PRISM_ROOT = {PRISM_ROOT}\")\n\n# === BUILTIN LENSES (no imports needed) ===\n\nclass BaseLens:\n    name = \"base\"\n    def analyze(self, panel): raise NotImplementedError\n    def top_indicators(self, result, n=10):\n        if 'importance' in result:\n            imp = result['importance']\n            if isinstance(imp, pd.Series):\n                return list(imp.sort_values(ascending=False).head(n).items())\n        return []\n\nclass MagnitudeLens(BaseLens):\n    \"\"\"Importance by total magnitude of movement (L2 norm).\"\"\"\n    name = \"magnitude\"\n    def analyze(self, panel):\n        normalized = (panel - panel.mean()) / panel.std()\n        magnitude = np.sqrt((normalized ** 2).sum())\n        return {'importance': magnitude, 'magnitude': magnitude}\n\nclass PCALens(BaseLens):\n    \"\"\"Importance by contribution to principal components.\"\"\"\n    name = \"pca\"\n    def analyze(self, panel, n_components=5):\n        X = ((panel - panel.mean()) / panel.std()).fillna(0)\n        U, S, Vt = np.linalg.svd(X.values, full_matrices=False)\n        explained = (S ** 2) / (len(X) - 1)\n        explained_ratio = explained / explained.sum()\n        loadings = Vt[:n_components].T * S[:n_components]\n        importance = pd.Series(np.abs(loadings).sum(axis=1), index=panel.columns)\n        return {'importance': importance, 'explained_variance_ratio': explained_ratio[:n_components],\n                'loadings': pd.DataFrame(loadings, index=panel.columns)}\n\nclass InfluenceLens(BaseLens):\n    \"\"\"Importance by volatility Ã— deviation.\"\"\"\n    name = \"influence\"\n    def analyze(self, panel, window=20):\n        vol = panel.rolling(window=window).std()\n        dev = np.abs(panel - panel.rolling(window=window).mean())\n        influence = (vol * dev).mean()\n        importance = (influence - influence.min()) / (influence.max() - influence.min())\n        return {'importance': importance, 'influence': influence}\n\nclass ClusteringLens(BaseLens):\n    \"\"\"Importance by centrality within correlation clusters.\"\"\"\n    name = \"clustering\"\n    def analyze(self, panel, n_clusters=None):\n        corr = panel.corr()\n        n_clusters = n_clusters or max(2, len(panel.columns) // 4)\n        dist = 1 - np.abs(corr.values)\n        np.random.seed(42)\n        centers = np.random.choice(len(panel.columns), n_clusters, replace=False)\n        labels = np.zeros(len(panel.columns), dtype=int)\n        for _ in range(10):\n            for i in range(len(panel.columns)):\n                labels[i] = np.argmin([dist[i, c] for c in centers])\n            for k in range(n_clusters):\n                members = np.where(labels == k)[0]\n                if len(members) > 0:\n                    centers[k] = members[np.argmin([dist[m, members].mean() for m in members])]\n        importance = pd.Series([1/(1+dist[i, np.where(labels==labels[i])[0]].mean()) \n                                 for i in range(len(panel.columns))], index=panel.columns)\n        return {'importance': importance, 'labels': pd.Series(labels, index=panel.columns)}\n\nclass CorrelationLens(BaseLens):\n    \"\"\"Importance by average absolute correlation with other indicators.\"\"\"\n    name = \"correlation\"\n    def analyze(self, panel):\n        corr = panel.corr().abs()\n        avg_corr = (corr.sum() - 1) / (len(corr) - 1)  # Exclude self-correlation\n        return {'importance': avg_corr, 'correlation_matrix': corr}\n\nclass VolatilityLens(BaseLens):\n    \"\"\"Importance by rolling volatility.\"\"\"\n    name = \"volatility\"\n    def analyze(self, panel, window=20):\n        vol = panel.rolling(window=window).std().mean()\n        importance = (vol - vol.min()) / (vol.max() - vol.min())\n        return {'importance': importance, 'volatility': vol}\n\nclass MomentumLens(BaseLens):\n    \"\"\"Importance by trend strength (recent vs historical).\"\"\"\n    name = \"momentum\"\n    def analyze(self, panel, lookback=60):\n        returns = panel.pct_change().dropna()\n        recent = returns.tail(lookback).mean()\n        historical = returns.mean()\n        momentum = (recent - historical).abs()\n        importance = (momentum - momentum.min()) / (momentum.max() - momentum.min())\n        return {'importance': importance, 'momentum': momentum}\n\n# === LENS REGISTRY ===\nLENSES = {\n    'magnitude': MagnitudeLens,\n    'pca': PCALens,\n    'influence': InfluenceLens,\n    'clustering': ClusteringLens,\n    'correlation': CorrelationLens,\n    'volatility': VolatilityLens,\n    'momentum': MomentumLens,\n}\n\ndef run_lens(name, panel):\n    return LENSES[name]().analyze(panel)\n\ndef run_all_lenses(panel, names=None):\n    names = names or list(LENSES.keys())\n    results = {}\n    for name in names:\n        try:\n            results[name] = run_lens(name, panel)\n            print(f\"  âœ“ {name}\")\n        except Exception as e:\n            print(f\"  âœ— {name}: {e}\")\n    return results\n\ndef compute_consensus(results):\n    rankings = {}\n    for name, res in results.items():\n        if 'importance' in res and isinstance(res['importance'], pd.Series):\n            rankings[name] = res['importance'].rank(ascending=False)\n    if not rankings: return pd.DataFrame()\n    df = pd.DataFrame(rankings)\n    df['avg_rank'] = df.mean(axis=1)\n    df['agreement'] = 1 / (1 + df.drop(columns=['avg_rank']).std(axis=1))\n    return df.sort_values('avg_rank')\n\nprint(f\"âœ“ Loaded {len(LENSES)} lenses: {list(LENSES.keys())}\")\nprint(\"\\nðŸ”‘ Ready to go!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title ðŸ“Š **LOAD DATA** { display-mode: \"form\" }\n#@markdown Loads and cleans your data.\n\n# Find data\ndata_file = PRISM_ROOT / 'data' / 'raw' / 'master_panel.csv'\n\nif not data_file.exists():\n    # Try to build from individual CSVs\n    print(\"Building panel from individual files...\")\n    raw_dir = PRISM_ROOT / 'data' / 'raw'\n    dfs = {}\n    for f in raw_dir.glob('*.csv'):\n        try:\n            df = pd.read_csv(f, index_col=0, parse_dates=True)\n            name = f.stem.upper()\n            if len(df.columns) >= 1:\n                # Use first numeric column or 'Close' or 'Value'\n                for col in ['Close', 'VALUE', 'Value', df.columns[0]]:\n                    if col in df.columns:\n                        dfs[name] = df[col]\n                        break\n        except: pass\n    panel_raw = pd.DataFrame(dfs)\nelse:\n    panel_raw = pd.read_csv(data_file, index_col=0, parse_dates=True)\n\n# Clean: forward fill, backward fill, drop remaining NaNs\npanel = panel_raw.ffill().bfill().dropna()\n\nprint(f\"âœ“ Loaded {panel.shape[1]} indicators, {panel.shape[0]} time points\")\nprint(f\"  Date range: {panel.index[0].strftime('%Y-%m-%d')} to {panel.index[-1].strftime('%Y-%m-%d')}\")\nprint(f\"  Indicators: {list(panel.columns)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ðŸš€ **RUN ANALYSIS** { display-mode: \"form\" }\n",
    "#@markdown Runs all lenses and computes consensus.\n",
    "\n",
    "print(\"Running all lenses...\\n\")\n",
    "results = run_all_lenses(panel)\n",
    "\n",
    "print(\"\\nComputing consensus...\")\n",
    "consensus = compute_consensus(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ðŸ“Š TOP 10 INDICATORS (by consensus)\")\n",
    "print(\"=\"*50)\n",
    "for i, (ind, row) in enumerate(consensus.head(10).iterrows(), 1):\n",
    "    bar = \"â–ˆ\" * int(row['agreement'] * 20)\n",
    "    print(f\"{i:2}. {ind:<15} rank={row['avg_rank']:.1f}  agreement={row['agreement']:.2f} {bar}\")\n",
    "\n",
    "print(\"\\nâœ“ Analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ðŸ“ˆ **VISUALIZE** { display-mode: \"form\" }\n",
    "#@markdown Creates charts showing results.\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Top indicators bar chart\n",
    "top10 = consensus.head(10)\n",
    "colors = plt.cm.RdYlGn(top10['agreement'].values)\n",
    "axes[0].barh(range(len(top10)), top10['avg_rank'].values[::-1], color=colors[::-1])\n",
    "axes[0].set_yticks(range(len(top10)))\n",
    "axes[0].set_yticklabels(top10.index[::-1])\n",
    "axes[0].set_xlabel('Average Rank (lower = more important)')\n",
    "axes[0].set_title('Top 10 Indicators by Consensus')\n",
    "axes[0].invert_xaxis()\n",
    "\n",
    "# Right: Lens agreement heatmap\n",
    "lens_cols = [c for c in consensus.columns if c not in ['avg_rank', 'agreement']]\n",
    "rank_matrix = consensus[lens_cols].head(10)\n",
    "im = axes[1].imshow(rank_matrix.values, cmap='RdYlGn_r', aspect='auto')\n",
    "axes[1].set_xticks(range(len(lens_cols)))\n",
    "axes[1].set_xticklabels(lens_cols, rotation=45, ha='right')\n",
    "axes[1].set_yticks(range(len(rank_matrix)))\n",
    "axes[1].set_yticklabels(rank_matrix.index)\n",
    "axes[1].set_title('Rank by Each Lens (green=high, red=low)')\n",
    "plt.colorbar(im, ax=axes[1], label='Rank')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Lens correlation\n",
    "print(\"\\nLens Agreement (Spearman correlation):\")\n",
    "lens_corr = consensus[lens_cols].corr(method='spearman')\n",
    "print(lens_corr.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title ðŸ’¾ **SAVE RESULTS** { display-mode: \"form\" }\n#@markdown Saves consensus to your output folder.\n\noutput_dir = PRISM_ROOT / '06_output' / 'latest'\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Save consensus\nconsensus.to_csv(output_dir / 'consensus.csv')\n\n# Save run metadata\nimport json\nmeta = {\n    'timestamp': datetime.now().isoformat(),\n    'data_shape': list(panel.shape),\n    'date_range': [str(panel.index[0]), str(panel.index[-1])],\n    'lenses_run': list(results.keys()),\n    'top_5': list(consensus.head(5).index),\n}\nwith open(output_dir / 'run_meta.json', 'w') as f:\n    json.dump(meta, f, indent=2)\n\nprint(f\"âœ“ Saved to {output_dir}/\")\nprint(f\"  - consensus.csv\")\nprint(f\"  - run_meta.json\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Quick Reference\n",
    "\n",
    "**Run a specific lens:**\n",
    "```python\n",
    "result = run_lens('pca', panel)\n",
    "print(result['importance'].sort_values(ascending=False))\n",
    "```\n",
    "\n",
    "**See what's in a result:**\n",
    "```python\n",
    "result = run_lens('clustering', panel)\n",
    "print(result.keys())  # Shows available outputs\n",
    "```\n",
    "\n",
    "**Available lenses:**\n",
    "- `magnitude` - Total movement (L2 norm)\n",
    "- `pca` - Principal component contribution  \n",
    "- `influence` - Volatility Ã— deviation\n",
    "- `clustering` - Correlation cluster centrality\n",
    "- `correlation` - Average correlation with others\n",
    "- `volatility` - Rolling volatility\n",
    "- `momentum` - Trend strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ SANDBOX - Play here!\n",
    "\n",
    "# Example: Look at PCA details\n",
    "pca_result = run_lens('pca', panel)\n",
    "print(\"Explained variance by component:\")\n",
    "print(pca_result['explained_variance_ratio'])\n",
    "print(f\"\\nTop loadings on PC1:\")\n",
    "print(pca_result['loadings'][0].sort_values(ascending=False).head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}